import requests
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from urllib.parse import urljoin
import sys
import argparse

def get_final_url(url, max_redirects=10, timeout=5):
    """
    è·å– URL çš„æœ€ç»ˆé‡å®šå‘åœ°å€ï¼Œå¹¶åœ¨è·å–åˆ°å“åº”å¤´åæ£€æŸ¥ Content-Typeã€‚
    å¦‚æœæ£€æµ‹åˆ°è§†é¢‘å†…å®¹ï¼Œåˆ™ä¸­æ­¢ä¸‹è½½å“åº”ä½“ã€‚
    """
    current_url = url
    redirect_count = 0

    try:
        while redirect_count < max_redirects:
            # åˆå§‹è¯·æ±‚ï¼Œallow_redirects=False æ¥æ‰‹åŠ¨å¤„ç†é‡å®šå‘
            response = requests.get(current_url, allow_redirects=False, timeout=timeout, stream=True) # stream=True å…³é”®
            response.raise_for_status() # æ£€æŸ¥HTTPçŠ¶æ€ç ï¼Œå¦‚æœä¸æ˜¯2xxï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸

            if response.status_code in (301, 302, 303, 307, 308) and 'Location' in response.headers:
                new_url = response.headers['Location']
                if not new_url.startswith(('http://', 'https://')):
                    new_url = urljoin(current_url, new_url)
                current_url = new_url
                redirect_count += 1
                # åœ¨é‡å®šå‘æ—¶å…³é—­å½“å‰å“åº”çš„è¿æ¥
                response.close()
            else:
                # åˆ°è¾¾æœ€ç»ˆURLï¼Œæˆ–è€…ä¸å†é‡å®šå‘
                final_url = current_url
                content_type = response.headers.get('Content-Type', '').lower()
                print(f"æœ€ç»ˆURL: {final_url}")
                print(f"Content-Type: {content_type}")

                if 'video/' in content_type or 'application/octet-stream' in content_type: # æŸäº›FLVå¯èƒ½æ²¡æœ‰æ˜ç¡®çš„video/flv
                    print("æ£€æµ‹åˆ°è§†é¢‘å†…å®¹ï¼Œä¸­æ­¢å“åº”ä½“ä¸‹è½½ã€‚")
                    response.close() # ç«‹å³å…³é—­è¿æ¥ï¼Œä¸­æ­¢ä¸‹è½½
                    return final_url, True, True # è¿”å›æœ€ç»ˆURLï¼ŒæˆåŠŸï¼Œæ˜¯è§†é¢‘
                else:
                    # å¦‚æœä¸æ˜¯è§†é¢‘ï¼Œä½ å¯ä»¥é€‰æ‹©ç»§ç»­å¤„ç†å“åº”ä½“ï¼Œæˆ–è€…ç›´æ¥å…³é—­
                    response.close() # å¦‚æœä¸éœ€è¦å“åº”ä½“å†…å®¹ï¼Œä¹Ÿå¯ä»¥ç›´æ¥å…³é—­
                    return final_url, True, False # è¿”å›æœ€ç»ˆURLï¼ŒæˆåŠŸï¼Œä¸æ˜¯è§†é¢‘

    except requests.exceptions.RequestException as e:
        print(f"âš ï¸ è¯·æ±‚å¤±è´¥: {current_url} ({type(e).__name__}: {e})")
        return current_url, False, False # è¿”å›å½“å‰URLï¼Œå¤±è´¥ï¼Œä¸æ˜¯è§†é¢‘

def resolve_urls_with_retry(urls, max_workers=10, timeout=5, max_retries=3, delay_between_retries=10):
    """
    è§£æURLï¼Œå¤±è´¥åå»¶è¿Ÿé‡è¯•ï¼Œæœ€å¤šå°è¯• max_retries æ¬¡
    """
    resolved_urls = {}
    retries = 0

    while retries <= max_retries:
        print(f"\nğŸ”„ å¼€å§‹ç¬¬ {retries+1} è½®å¤„ç†...")
        failed_urls = []

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_url = {executor.submit(get_final_url, url, 10, timeout): url for url in urls}

            for future in as_completed(future_to_url):
                url = future_to_url[future]
                final_url, success = future.result()
                resolved_urls[url] = final_url
                if success:
                    print(f"âœ… æˆåŠŸ: {final_url}")
                else:
                    print(f"âŒ å¤±è´¥: {url}")
                    failed_urls.append(url)

        if not failed_urls:
            break  # å…¨éƒ¨æˆåŠŸï¼Œè·³å‡ºå¾ªç¯
        if retries == max_retries:
            print("\nâ—å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œä»¥ä¸‹ URL ä»å¤„ç†å¤±è´¥ï¼š")
            for url in failed_urls:
                print(url)
            break

        print(f"\nâ³ ç­‰å¾… {delay_between_retries} ç§’åé‡æ–°å°è¯• {len(failed_urls)} ä¸ªå¤±è´¥çš„è¯·æ±‚...")
        time.sleep(delay_between_retries)
        urls = failed_urls
        retries += 1

    return resolved_urls

def process_m3u_file(input_file, output_file, max_workers=10, timeout=5, max_retries=3):
    """
    å¤„ç† M3U æ–‡ä»¶ï¼Œè§£ææ‰€æœ‰ URLï¼Œè‡ªåŠ¨é‡è¯•å¤±è´¥é¡¹
    """
    start_time = time.time()

    with open(input_file, 'r', encoding='utf-8') as f:
        lines = [line.strip() for line in f.readlines()]

    url_pattern = re.compile(r'^https?://\S+')
    url_to_line_indices = {}
    urls = []

    for i, line in enumerate(lines):
        if url_pattern.match(line):
            urls.append(line)
            url_to_line_indices.setdefault(line, []).append(i)

    resolved_map = resolve_urls_with_retry(
        urls, max_workers=max_workers, timeout=timeout, max_retries=max_retries, delay_between_retries=10
    )

    for original_url, final_url in resolved_map.items():
        for i in url_to_line_indices[original_url]:
            lines[i] = final_url

    with open(output_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))

    print(f"\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œæ€»è€—æ—¶ {time.time() - start_time:.2f} ç§’")
    print(f"è¾“å…¥æ–‡ä»¶: {input_file}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")

def parse_arguments():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°
    """
    parser = argparse.ArgumentParser(description='å¤„ç†M3Uæ–‡ä»¶ä¸­çš„URLé‡å®šå‘')
    parser.add_argument('--input', required=True, help='è¾“å…¥M3Uæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', required=True, help='è¾“å‡ºM3Uæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--workers', type=int, default=5, help='æœ€å¤§å·¥ä½œçº¿ç¨‹æ•° (é»˜è®¤: 5)')
    parser.add_argument('--timeout', type=int, default=10, help='è¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’) (é»˜è®¤: 10)')
    parser.add_argument('--retries', type=int, default=5, help='æœ€å¤§é‡è¯•æ¬¡æ•° (é»˜è®¤: 5)')
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_arguments()
    process_m3u_file(
        input_file=args.input,
        output_file=args.output,
        max_workers=args.workers,
        timeout=args.timeout,
        max_retries=args.retries
    )
