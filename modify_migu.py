import requests
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from urllib.parse import urljoin

def get_final_url(url, max_redirects=10, timeout=5):
    """
    è·å– URL çš„æœ€ç»ˆé‡å®šå‘åœ°å€
    :param url: åŸå§‹ URL
    :param max_redirects: æœ€å¤§é‡å®šå‘æ¬¡æ•°
    :param timeout: è¶…æ—¶æ—¶é—´
    :return: æœ€ç»ˆ URL æˆ– Noneï¼ˆå¦‚æœå¤±è´¥ï¼‰
    """
    try:
        response = requests.get(url, allow_redirects=False, timeout=timeout)
        redirect_count = 0

        while redirect_count < max_redirects:
            if response.status_code in (301, 302, 303, 307, 308) and 'Location' in response.headers:
                new_url = response.headers['Location']
                if not new_url.startswith(('http://', 'https://')):
                    new_url = urljoin(url, new_url)
                url = new_url
                response = requests.get(url, allow_redirects=False, timeout=timeout)
                redirect_count += 1
            else:
                break

        return url
    except requests.RequestException as e:
        print(f"âš ï¸ è¯·æ±‚å¤±è´¥: {url} ({type(e).__name__}: {e})")
        return None  # å¤±è´¥æ—¶è¿”å› None

def resolve_urls(lines, url_pattern, max_workers=10, timeout=5):
    """
    å¤šçº¿ç¨‹è§£æ URL å¹¶è¿”å›æ–°çš„è¡Œå†…å®¹å’Œå¤±è´¥çš„ URL æ˜ å°„
    """
    new_lines = lines[:]
    failed = {}

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {}
        for i, line in enumerate(lines):
            if url_pattern.match(line):
                future = executor.submit(get_final_url, line, 10, timeout)
                futures[future] = i

        for future in as_completed(futures):
            idx = futures[future]
            result = future.result()
            if result:
                new_lines[idx] = result
                print(f"âœ… æˆåŠŸ: {result}")
            else:
                failed[idx] = lines[idx]
                print(f"âŒ å¤±è´¥: {lines[idx]}")

    return new_lines, failed

def process_m3u_file(input_file, output_file, max_workers=10, timeout=5):
    start_time = time.time()

    with open(input_file, 'r', encoding='utf-8') as f:
        lines = [line.strip() for line in f.readlines()]

    url_pattern = re.compile(r'^https?://\S+')

    # åˆæ¬¡è¯·æ±‚
    lines, failed = resolve_urls(lines, url_pattern, max_workers, timeout)

    # å¾ªç¯é‡è¯•å¤±è´¥çš„è¯·æ±‚ï¼Œç›´åˆ°æ²¡æœ‰å¤±è´¥ä¸ºæ­¢
    retry_count = 10
    while failed:
        retry_count += 1
        print(f"\nğŸ” ç¬¬ {retry_count} æ¬¡é‡è¯•ä¸­ï¼Œå…± {len(failed)} ä¸ªå¤±è´¥è¯·æ±‚...")
        time.sleep(10)

        # å‡†å¤‡ retry_linesï¼Œä»…æ›¿æ¢ failed éƒ¨åˆ†
        retry_lines = [failed[i] if i in failed else '' for i in range(len(lines))]
        retry_result, retry_failed = resolve_urls(retry_lines, url_pattern, max_workers, timeout)

        # åˆå¹¶ retry_result åˆ°ä¸» lines ä¸­
        for i in failed:
            if retry_result[i]:
                lines[i] = retry_result[i]

        # æ›´æ–°å¤±è´¥è®°å½•
        failed = {i: failed[i] for i in failed if retry_result[i] is None}

    # å†™å…¥æœ€ç»ˆæ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))

    print(f"\nğŸ‰ å…¨éƒ¨å¤„ç†å®Œæˆï¼æ€»è€—æ—¶ {time.time() - start_time:.2f} ç§’")
    print(f"åŸå§‹æ–‡ä»¶: {input_file} â†’ è¾“å‡ºæ–‡ä»¶: {output_file}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    input_m3u = "migu.m3u"
    output_m3u = "final.m3u"
    process_m3u_file(input_m3u, output_m3u, max_workers=5, timeout=10)
