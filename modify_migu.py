import requests
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urljoin
import time
from tqdm import tqdm  # pip install tqdm
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(filename='errors.log', level=logging.WARNING, format='%(asctime)s - %(message)s')

def get_final_url(url, max_redirects=10, connect_timeout=3, read_timeout=5, retries=3, delay=0.2):
    """
    è·å– URL çš„æœ€ç»ˆé‡å®šå‘åœ°å€ï¼Œæ”¯æŒå¤±è´¥é‡è¯•ä¸é™é€Ÿæ§åˆ¶ã€‚
    """
    for attempt in range(1, retries + 1):
        try:
            response = requests.get(url, allow_redirects=False, timeout=(connect_timeout, read_timeout))
            redirect_count = 0

            while redirect_count < max_redirects:
                if response.status_code in (301, 302, 303, 307, 308) and 'Location' in response.headers:
                    new_url = response.headers['Location']
                    if not new_url.startswith(('http://', 'https://')):
                        new_url = urljoin(url, new_url)
                    url = new_url
                    response = requests.get(url, allow_redirects=False, timeout=(connect_timeout, read_timeout))
                    redirect_count += 1
                else:
                    break

            if delay > 0:
                time.sleep(delay)
            return url

        except requests.RequestException as e:
            if attempt == retries:
                logging.warning(f"è¯·æ±‚å¤±è´¥ [{url}] (ç¬¬ {attempt} æ¬¡): {e}")
                return url  # å¤±è´¥æ—¶è¿”å›åŸå§‹ URL
            time.sleep(delay)  # ç­‰å¾…åé‡è¯•

def process_m3u_file(input_file, output_file, max_workers=10, connect_timeout=3, read_timeout=5, delay=0.2):
    """
    å¤„ç† M3U æ–‡ä»¶ï¼Œå¤šçº¿ç¨‹ã€é™é€Ÿã€é‡è¯•ã€é”™è¯¯æ—¥å¿—ç­‰å¢å¼ºåŠŸèƒ½ã€‚
    """
    start_time = time.time()

    with open(input_file, 'r', encoding='utf-8') as f:
        lines = [line.strip() for line in f.readlines()]

    url_pattern = re.compile(r'^https?://\S+')
    url_tasks = []

    # å‡†å¤‡ä»»åŠ¡
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {}
        for i, line in enumerate(lines):
            if url_pattern.match(line):
                future = executor.submit(
                    get_final_url,
                    line,
                    max_redirects=10,
                    connect_timeout=connect_timeout,
                    read_timeout=read_timeout,
                    retries=3,
                    delay=delay
                )
                futures[future] = i

        # æ˜¾ç¤ºè¿›åº¦
        for future in tqdm(as_completed(futures), total=len(futures), desc="ğŸ” æ­£åœ¨å¤„ç†é“¾æ¥"):
            i = futures[future]
            lines[i] = future.result()

    with open(output_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))

    print(f"\nâœ… æ‰€æœ‰é“¾æ¥å¤„ç†å®Œæˆï¼Œè€—æ—¶ {time.time() - start_time:.2f} ç§’")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"å¦‚æœ‰é”™è¯¯è®°å½•ï¼Œè¯·æŸ¥çœ‹ errors.log")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    input_m3u = "migu.m3u"
    output_m3u = "final.m3u"
    process_m3u_file(input_m3u, output_m3u, max_workers=5, connect_timeout=5, read_timeout=5, delay=0.3)
